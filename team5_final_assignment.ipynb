{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npFNumwXqm-v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding ,  Permute,Dropout, AvgPool2D , BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN , Reshape ,GlobalAvgPool1D , GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense , Bidirectional , LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.regularizers import l2\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearningCurve(history,epochs):\n",
    "  \"\"\"\n",
    "  Plot accuracy chart vs number of epoch performed\n",
    "    history : store model performance data\n",
    "    epochs : stores number of epochs\n",
    "  Return : Graph\n",
    "  \"\"\"\n",
    "  epochRange = range(1,epochs+1)\n",
    "  plt.plot(epochRange,history.history['categorical_accuracy'])\n",
    "  plt.plot(epochRange,history.history['val_categorical_accuracy'])\n",
    "  plt.title('Model Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend(['Train','Validation'],loc='best')\n",
    "  plt.show()\n",
    "\n",
    "  plt.plot(epochRange,history.history['loss'])\n",
    "  plt.plot(epochRange,history.history['val_loss'])\n",
    "  plt.title('Model Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend(['Train','Validation'],loc='best')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy(predictions, y_test):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for test cases\n",
    "        predictions : Stores predicted categories\n",
    "        y_test : Stores actual categories\n",
    "    Return : accuracy as float datatype\n",
    "    \"\"\"\n",
    "    # Get predicted class indices\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_background_noise(audio, output_file_path, noise_level=0.2):\n",
    "    \"\"\"\n",
    "    Add background noise to actual data for data augmentation.\n",
    "        audio : actual audio data\n",
    "        output_file_path : file path address to store augemented data for manual testing\n",
    "        noise_level : supression metric for added noice\n",
    "\n",
    "        formula : audio + background_noise * noise_level\n",
    "    \n",
    "    return : augmented data as list of list datatype\n",
    "    \"\"\"\n",
    "    noise_files = os.listdir()\n",
    "    noise_files = [x for x in noise_files if x != '.DS_Store' and x.endswith('.wav')]\n",
    "    augmented = []\n",
    "    for noise_data in noise_files:\n",
    "        y, sr = librosa.load(noise_data, sr=20000)\n",
    "        noise = y.astype(np.float32)\n",
    "\n",
    "        # Adjust noise volume\n",
    "        # noise = noise - abs(noise_level)  # Make noise quieter\n",
    "\n",
    "        # Loop or trim noise to match audio length\n",
    "        if len(noise) < len(audio):\n",
    "            noise = np.pad(noise, (0, len(audio) - len(noise)), 'wrap')\n",
    "        else:\n",
    "            noise = noise[:len(audio)]\n",
    "\n",
    "        # Combine audio and noise\n",
    "        augmented_audio = audio + noise_level * noise\n",
    "        augmented.append(augmented_audio)\n",
    "\n",
    "        # Export augmented audio\n",
    "        sf.write(output_file_path, augmented_audio, sr)\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3287rQmWr_-7"
   },
   "outputs": [],
   "source": [
    "def sliding_window(arr, window_size, step_size):\n",
    "    for i in range(0, len(arr) - window_size + 1, step_size):\n",
    "        for elem in arr[i:i + window_size]:\n",
    "            yield elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_average(signal, window_size):\n",
    "    if len(signal) > window_size:\n",
    "        window = np.ones(window_size) / window_size\n",
    "        smoothed_signal = np.convolve(signal, window, mode='same')\n",
    "        return smoothed_signal\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_shift(audio):\n",
    "    shift_factor = np.random.uniform(0.9, 1.1)\n",
    "    # Apply the amplitude shift by multiplying the audio with the shift factor\n",
    "    shifted_audio = audio * shift_factor\n",
    "    return shifted_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeQt8a7esAu2"
   },
   "outputs": [],
   "source": [
    "def apply_hamming_window(windows):\n",
    "\n",
    "    window_size = len(windows)  # Get the size of each window\n",
    "\n",
    "    hamming = np.hamming(window_size)  # Create Hamming window\n",
    "\n",
    "    # Reshape to (1, window_size) for broadcasting along each row (window)\n",
    "    hamming = hamming.reshape(1, -1)  # Changed to (1, window_size)\n",
    "\n",
    "    # Apply the Hamming window to each window\n",
    "    # windows_hamming = windows * hamming\n",
    "    windows_hamming = windows\n",
    "\n",
    "\n",
    "    return windows_hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_standard_scalar(audio_data):\n",
    "    \"\"\"\n",
    "    Apply standard scaler to fit data to standard normal form\n",
    "        audio_data : stores augemented/preprocessed audio data\n",
    "\n",
    "    return : scaled data as list\n",
    "    \"\"\"\n",
    "    if audio_data.shape[1] > 0:\n",
    "        scaled_data = scaler.fit_transform(audio_data)\n",
    "        return scaled_data\n",
    "    return audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9E9BUnjsF8E"
   },
   "outputs": [],
   "source": [
    "def process_audio_files(main_folder):\n",
    "    processed_data = {}\n",
    "    folders = os.listdir(main_folder)\n",
    "    folders = [x for x in folders if x != '.DS_Store']\n",
    "    print(folders)\n",
    "\n",
    "    total_files = sum(len(os.listdir(os.path.join(main_folder, folder))) for folder in folders)\n",
    "    with tqdm(total=total_files, desc='Processing Audio Files', unit='file') as pbar:\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(main_folder, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                processed_data[folder] = []\n",
    "                for file_name in os.listdir(folder_path):\n",
    "                    if file_name.endswith('.wav'):\n",
    "                        file_path = os.path.join(folder_path, file_name)\n",
    "                        y, sr = librosa.load(file_path, sr=20000)\n",
    "                        original_data = y.astype(np.float32)\n",
    "\n",
    "                        # Apply augmentations\n",
    "                        shifted_data = amplitude_shift(original_data)#randon int\n",
    "                        noisy_data_list = add_background_noise(shifted_data , os.path.join('Output' ,folder, 'augmented_' + file_name))\n",
    "\n",
    "                        # Process each augmented data variant\n",
    "                        for variant_name, variant_data in zip(\n",
    "                            ['original'] + [f'augmented_{i}' for i in range(len(noisy_data_list))],\n",
    "                            [original_data] + noisy_data_list):\n",
    "                            \n",
    "                            # Sliding window\n",
    "                            # windowed_data = sliding_window(variant_data, 30000, 5000)\n",
    "                            # windows = sliding_window_average(variant_data, int(sr * 0.1))\n",
    "\n",
    "                            # Hamming window\n",
    "                            windows_hamming = apply_hamming_window(list(variant_data))\n",
    "\n",
    "                            # Standard Scalar\n",
    "                            # scaled_data = apply_standard_scalar(windows_hamming)\n",
    "\n",
    "                            # Store processed data\n",
    "                            processed_data[folder].append({\n",
    "                                'label': folder,\n",
    "                                'variant': variant_name,\n",
    "                                'data': windows_hamming\n",
    "                            })\n",
    "                    pbar.update(1)\n",
    "\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtAcbr5dsIhC",
    "outputId": "2c9da154-3a78-4b14-9a43-dc592e5d87d2"
   },
   "outputs": [],
   "source": [
    "# Define the main folder path\n",
    "main_folder = 'Original'\n",
    "\n",
    "# Process all audio files with augmentations, sliding window, hamming, and scaling\n",
    "final_data = process_audio_files(main_folder)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "itf6LvCI_PDO",
    "outputId": "76c87ce2-29ee-458d-b795-7843695b97c5"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['label', 'variant', 'data'])\n",
    "\n",
    "for label, variants in final_data.items():\n",
    "    for variant in variants:\n",
    "        df = pd.concat([df, pd.DataFrame([{'label': label, 'variant': variant['variant'], 'data': variant['data']}])], ignore_index=True)\n",
    "\n",
    "df\n",
    "# df['data'] = df['data'].apply(lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNhYwQlT_wJZ"
   },
   "outputs": [],
   "source": [
    "# df.to_csv('final_data.csv', index=False) # Save in local for manual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Swn3EGjZJB7x"
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNnpDMPbGvFC"
   },
   "outputs": [],
   "source": [
    "# Encode catogerical field \n",
    "le = LabelEncoder()\n",
    "le.fit(df['label'])\n",
    "df['label'] = le.transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUedFok4Ixgt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set dependent and independent field\n",
    "    x : independent\n",
    "    y : dependent\n",
    "'''\n",
    "x = df['data']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pad the data to same lenght to convert to tensor form.\n",
    "Standarize encoded field to integer datatype.\n",
    "'''\n",
    "x = sequence.pad_sequences(x, maxlen=30225, padding='post', truncating='post', dtype='float32')\n",
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2) # Split data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert the data to tensor format.\n",
    "'''\n",
    "x_train = x_train.reshape((x_train.shape[0],1, 30225, 1))  # (batch_size, height, width, channels)\n",
    "x_test = x_test.reshape((x_test.shape[0],1, 30225 , 1))\n",
    "y_train = to_categorical(y_train, num_classes=8)\n",
    "y_test = to_categorical(y_test, num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "Z4ICYcTmL-SH",
    "outputId": "21cceb1a-4458-4f0a-8df9-7d82d6e3cd68"
   },
   "outputs": [],
   "source": [
    "model_cnn = Sequential([\n",
    "    # Embedding(input_dim=50000 , output_dim=128, input_length=500),\n",
    "    #SFEB\n",
    "    Conv2D(8, (1,9), strides = (1,2) , activation='relu', input_shape= (1 , 30225 , 1)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (1,5), strides = (1,2),  activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size = (1, 50), strides = (1, 50)),\n",
    "    Permute((3, 2, 1)), #SwapAxes\n",
    "    #TFEB\n",
    "    Conv2D(32 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    Conv2D(64 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(128 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    Conv2D(128 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(256 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    Conv2D(256 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(512 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(512 , (3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(8 , (1,1) , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    AvgPool2D((1,4)),\n",
    "    Flatten(),\n",
    "    Dense(8 , activation = 'relu'),\n",
    "    Dense(8 , activation = 'softmax')#Output\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(model_cnn,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_cnn.fit(x_train , y_train , epochs = 50 , validation_data = (x_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(model,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.save(\"model_cnn_without_quantization.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real time accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing real time accuracy by passing test data to saved model.\n",
    "##### Ideally newly collected data is used for real time testing but due to lack of resources and data, test data was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = model_cnn.input_shape\n",
    "output_details = model_cnn.output_shape\n",
    "print(input_details)\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_cnn.predict(x_test)\n",
    "\n",
    "# If using one-hot encoded labels, convert predictions to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Convert to class indices\n",
    "true_classes = np.argmax(y_test, axis=1)  # Convert true labels to class indices\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is Quantization?\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\") # Quantize the model and save as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "interpreter.allocate_tensors()\n",
    "print(input_details)\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for img in x_test:  \n",
    "    interpreter.set_tensor(input_details[0]['index'], [img.astype('float32')])\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions.append(output_data[0])\n",
    "predictions = np.array(predictions)\n",
    "get_test_accuracy(predictions, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_cnn)\n",
    "# Enable full integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Convert the model\n",
    "quantized_model = converter.convert()\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_rnn = Sequential([\n",
    "    # Embedding(input_dim=50000 , output_dim=128, input_length=500),\n",
    "    #SFEB\n",
    "    Conv2D(8, (1,9), strides = (1,2) , activation='relu', input_shape= (1 , 30225,1)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (1,5), strides = (1,2),  activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((1, 50)),\n",
    "    Permute((3, 2, 1)), #SwapAxes\n",
    "    #TFEB\n",
    "    Conv2D(32 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    Conv2D(64 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(128 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    Conv2D(128 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(256 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    Conv2D(256 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(512 ,(3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(512 , (3,3) , padding = 'same' , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32 , (1,1) , activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    AvgPool2D((1,4)),\n",
    "    Reshape((2, 32)),\n",
    "    LSTM(8 , activation='relu', return_sequences=True), # LSTM layer \n",
    "    BatchNormalization(),\n",
    "    GlobalAvgPool1D(),\n",
    "    # SimpleRNN(8 , activation='relu', return_sequences=True),\n",
    "    # BatchNormalization(),\n",
    "    # GlobalAvgPool1D(),\n",
    "    Dense(8 , activation = 'relu'),\n",
    "    Dense(8 , activation = 'softmax')#Output\n",
    "])\n",
    "\n",
    "model_cnn_rnn .compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_cnn_rnn .summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model_cnn_rnn.fit(x_train , y_train , epochs = 50 , validation_data = (x_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(model_2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_rnn.save(\"model_cnn_rnn_without_quantization.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_cnn_rnn.predict(x_test)\n",
    "\n",
    "# If using one-hot encoded labels, convert predictions to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Convert to class indices\n",
    "true_classes = np.argmax(y_test, axis=1)  # Convert true labels to class indices\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_cnn = Sequential([\n",
    "    # Embedding(input_dim=50000 , output_dim=128, input_length=500),\n",
    "    #SFEB\n",
    "    Conv2D(7, (1,9), strides = (1,2) , activation='relu', input_shape= (1 , 30225,1), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(20, (1,5), strides = (1,2),  activation='relu', kernel_regularizer=l2(0.015)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((1, 50), strides = (1,50)),\n",
    "    Permute((3,2,1)), #SwapAxes\n",
    "    #TFEB\n",
    "    Conv2D(10 ,(3,3), strides = (1,1) , padding = 'same' , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2) , strides = (2,2)),\n",
    "    Conv2D(14 ,(3,3), strides = (1,1) , padding = 'same' , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    Conv2D(22 ,(3,3), strides = (1,1) , padding = 'same' , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2), strides = (2,2)),\n",
    "    Conv2D(31 ,(3,3), strides = (1,1) , padding = 'same' , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    Conv2D(35 ,(3,3), strides = (1,1) , padding = 'same' , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2), strides = (2,2)),\n",
    "    Conv2D(41 ,(3,3), strides = (1,1) , padding = 'same' , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    Conv2D(69 ,(3,3), strides = (1,1) , padding = 'same' , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2), strides = (2,2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(8 , (1,1), strides = (1,1) , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    AvgPool2D((1,4), strides = (1,4)),\n",
    "    Flatten(),\n",
    "    # Reshape((-1, 8)),\n",
    "    # LSTM(8 , activation='relu', return_sequences=True),\n",
    "    # BatchNormalization(),\n",
    "    # GlobalAvgPool1D(),\n",
    "    Dense(8 , activation = 'relu'),\n",
    "    Dense(8 , activation = 'softmax')#Output\n",
    "])\n",
    "\n",
    "pruned_model_cnn.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['categorical_accuracy'])\n",
    "pruned_model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned = pruned_model_cnn.fit(x_train , y_train , epochs = 50 , validation_data = (x_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(model_pruned,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pruned_model_cnn.predict(x_test)\n",
    "\n",
    "# If using one-hot encoded labels, convert predictions to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Convert to class indices\n",
    "true_classes = np.argmax(y_test, axis=1)  # Convert true labels to class indices\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pranit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
